# -----------------------------------------------------------------------------
# AI Tech Support Agent - Environment Variables Example
#
# Copy this file to .env and fill in your desired values.
# Lines starting with '#' are comments and will be ignored.
# -----------------------------------------------------------------------------

# --- Application Logging Configuration ---
# LOG_LEVEL: Controls the verbosity of application logs.
# Recommended values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
# LOG_LEVEL=INFO

# --- Embedding Model Configuration ---
# MODEL_NAME: The name of the Sentence Transformer model to be used.
# This model will be downloaded from Hugging Face models hub if not available locally.
# Ensure EMBEDDING_DIMENSION matches the chosen model.
# Default: "sentence-transformers/all-MiniLM-L6-v2"
# Examples:
# MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2"
# MODEL_NAME="sentence-transformers/all-mpnet-base-v2"
# MODEL_NAME="sentence-transformers/paraphrase-MiniLM-L3-v2"
MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# EMBEDDING_DIMENSION: The dimensionality of the embeddings produced by MODEL_NAME.
# MUST match the output dimension of the specified Sentence Transformer model.
# - "sentence-transformers/all-MiniLM-L6-v2": 384
# - "sentence-transformers/paraphrase-MiniLM-L3-v2": 384
# - "sentence-transformers/all-mpnet-base-v2": 768
# Default: 384 (for all-MiniLM-L6-v2)
EMBEDDING_DIMENSION=384

# --- Vector Store Configuration ---
# VECTOR_STORE_INDEX_DIR: The directory where the FAISS index and its metadata
# will be saved and loaded from. This path is relative to the project root.
# Default: "./data/vector_store"
VECTOR_STORE_INDEX_DIR=./data/vector_store

# TOP_K_RESULTS: The default number of top relevant document chunks to retrieve
# during a search operation if not specified in the query.
# Default: 5
TOP_K_RESULTS=5

# --- FastAPI Application Configuration ---
# API_HOST: The host address for the FastAPI application.
# "0.0.0.0" makes it accessible from other machines on the network.
# Default: "0.0.0.0"
API_HOST=0.0.0.0

# API_PORT: The port number for the FastAPI application.
# Default: 8000
API_PORT=8000

# --- LLM Configuration ---
# LLM_MODEL_NAME: The Hugging Face model name for the language model.
# Default: "google/flan-t5-base"
# Examples: "distilgpt2", "gpt2", "EleutherAI/gpt-neo-1.3B" (larger model)
LLM_MODEL_NAME=google/flan-t5-base

# LLM_MAX_NEW_TOKENS: Maximum number of new tokens the LLM can generate.
# Default: 250
LLM_MAX_NEW_TOKENS=250

# LLM_TEMPERATURE: Controls the randomness of the LLM's output.
# Lower values (e.g., 0.2) are more deterministic, higher values (e.g., 0.8) are more creative.
# Default: 0.7
LLM_TEMPERATURE=0.7

# LLM_PROMPT_TEMPLATE: Override the default prompt template defined in app/config.py.
# This is often a long multi-line string. For significant changes, it might be better
# to modify the DEFAULT_PROMPT_TEMPLATE in app/config.py directly or load from a separate file.
# If you set this, ensure it includes '{context_chunks}' and '{question}' placeholders.
# Example (if you were to override, though it's usually left blank to use the config default):
# LLM_PROMPT_TEMPLATE="Context: {context_chunks}\n\nQuestion: {question}\n\nAnswer:"
LLM_PROMPT_TEMPLATE=


# --- Optional: Hugging Face Cache ---
# HF_HOME: Path to the Hugging Face cache directory where models and datasets are stored.
# If not set, Hugging Face defaults to ~/.cache/huggingface.
# Setting this can be useful in environments with limited home directory space or for shared caches.
# Example:
# HF_HOME=/path/to/your/hf_cache_directory
